{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbf5868-7b21-47d0-a96f-1333caf46639",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b47da2-8474-4a35-9b83-e6a77985af16",
   "metadata": {},
   "source": [
    "An activation function in artificial neural networks is a mathematical function applied to the output of a neuron (or node). It determines whether a neuron should be activated or not, based on the weighted sum of its inputs. Activation functions introduce non-linearity into the network, allowing it to model complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae785f9-1bd7-49b2-8455-77d60c11a35e",
   "metadata": {},
   "source": [
    "## Q.2 \n",
    "\n",
    "1. softmax\n",
    "2. Leky Relu\n",
    "3. Relu\n",
    "4. Sigmoid \n",
    "5. Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2c26f-ad07-40de-99a9-9f068629bc4e",
   "metadata": {},
   "source": [
    "# Q.3 \n",
    "\n",
    "Activation functions impact the training process and performance of a neural network by:\n",
    "\n",
    "Introducing Non-linearity: Allowing the network to learn complex patterns and representations.\n",
    "Gradient Flow: Affecting how gradients are backpropagated through the network, influencing the speed and effectiveness of training.\n",
    "Output Range: Defining the range of output values, which can impact how the network weights are updated.\n",
    "Convergence: Influencing the network's ability to converge to a solution and avoid problems like vanishing or exploding gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334c767-9c77-4070-b577-6920bff38fa7",
   "metadata": {},
   "source": [
    "# Q.4 \n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Smooth Gradient: Provides a smooth gradient, which is beneficial for gradient-based optimization algorithms.\n",
    "Output Range: Outputs values in the range (0, 1), which can be interpreted as probabilities.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Vanishing Gradient: Gradients can become very small for large positive or negative inputs, slowing down learning.\n",
    "Not Zero-centered: Outputs are always positive, which can lead to inefficient updates in gradient descent.\n",
    "Computationally Expensive: The exponential function involved can be computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cc4f0-df36-4620-8505-04df00b16591",
   "metadata": {},
   "source": [
    "# Q.5 \n",
    "\n",
    "Differences from Sigmoid:\n",
    "\n",
    "Simplicity: ReLU is computationally simpler since it involves only a comparison operation.\n",
    "Non-linearity: Both introduce non-linearity, but ReLU does so in a piecewise linear manner.\n",
    "Gradient Behavior: ReLU does not suffer from the vanishing gradient problem as severely as sigmoid, especially for positive inputs.\n",
    "Output Range: ReLU outputs values in the range [0, âˆž), whereas sigmoid outputs values in (0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f03e3-e728-442d-9a0a-021544171ca9",
   "metadata": {},
   "source": [
    "# Q.6 \n",
    "\n",
    "Benefits of ReLU:\n",
    "\n",
    "Efficiency: Computationally efficient due to its simple mathematical operation.\n",
    "Mitigates Vanishing Gradient Problem: Helps in reducing the vanishing gradient issue, allowing for better gradient propagation and faster convergence.\n",
    "Sparse Activation: Leads to sparse activation (many neurons output zero), which can improve model efficiency and representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151600f-eaac-4c72-9a92-cad89c783d58",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "Leky Relu - \n",
    "\n",
    "Concept:\n",
    "Leaky ReLU introduces a small slope for negative input values instead of setting them to zero. This ensures that the gradient is never zero, addressing the issue of \"dying ReLUs\" where neurons output zero for all inputs.\n",
    "\n",
    "Addressing Vanishing Gradient:\n",
    "By allowing a small gradient when the input is negative, leaky ReLU ensures that gradients can still flow through the network, thereby mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3084ed-bd5f-4387-b589-20d2930afd42",
   "metadata": {},
   "source": [
    "# Q.8 \n",
    "\n",
    "Softmax\n",
    "\n",
    "Purpose:\n",
    "The softmax function converts a vector of values into a probability distribution, where each value represents the probability of belonging to a particular class. The sum of all probabilities equals 1.\n",
    "\n",
    "Common Usage:\n",
    "\n",
    "Output Layer: Commonly used in the output layer of classification networks, particularly for multi-class classification problems.\n",
    "Probabilistic Interpretation: It provides a probabilistic interpretation of the output, which is useful for tasks where the model's confidence in its predictions is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd5227-a8ec-4758-b877-a6e9bb3c5d79",
   "metadata": {},
   "source": [
    "# Q.9 \n",
    "\n",
    "Tanh - \n",
    "\n",
    "Comparison to Sigmoid:\n",
    "\n",
    "Output Range: Tanh outputs values in the range (-1, 1), while sigmoid outputs values in (0, 1).\n",
    "Zero-centered: Tanh is zero-centered, meaning its outputs can be positive or negative, leading to more balanced updates during training compared to sigmoid.\n",
    "Gradient Behavior: Like sigmoid, tanh can suffer from the vanishing gradient problem, but its zero-centered nature can help mitigate this issue to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f01a27-510d-4972-b53f-99eed7cf3e7f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
