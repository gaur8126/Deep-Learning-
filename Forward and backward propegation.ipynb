{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1174b5-6462-460f-96cf-248b7a7fd7e3",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562245c-d792-4afa-b6d2-ae3b68b8bf59",
   "metadata": {},
   "source": [
    "Forward propagation is the process of passing input data through a neural network to generate an output. The purpose of forward propagation is to compute the network's predictions, given the input data, by applying weights, biases, and activation functions across the network's layers. In essence, forward propagation determines how well the neural network is performing on a given task based on its current parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f0c9c-f7a1-4703-9cee-29cc9ec97aa9",
   "metadata": {},
   "source": [
    "# Q.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e58664-c6ce-44c9-8e34-a1d77264647e",
   "metadata": {},
   "source": [
    "In a single-layer feedforward neural network (also known as a single-layer perceptron), forward propagation is implemented as follows:\n",
    "\n",
    "Input: The input features X are fed into the network.\n",
    "Weighted Sum: The input is multiplied by the weights W and added to the bias b. This can be expressed mathematically as:\n",
    "Z = X.W+b\n",
    "\n",
    "Activation Function: The weighted sum \n",
    "Z is passed through an activation function f(Z) to introduce non-linearity. The output of the activation function is the final output of the layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be526f-dca5-49fc-a7a8-bf40ce560130",
   "metadata": {},
   "source": [
    "# Q.3\n",
    "\n",
    "Activation functions are applied to the weighted sum of inputs in each layer during forward propagation. Their primary purpose is to introduce non-linearity into the model, enabling the network to learn complex patterns and relationships in the data. Without activation functions, the neural network would behave like a simple linear regression model, limiting its ability to solve complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad2ecb-b106-400a-a46f-317fc5d84695",
   "metadata": {},
   "source": [
    "# Q.4\n",
    "\n",
    "Weights and biases are the key parameters that the neural network learns during training.\n",
    "\n",
    "Weights: Weights determine the strength and direction of the influence that each input feature has on the neuron's output. They scale the input data and allow the network to make more complex decisions based on the relationships between different inputs.\n",
    "\n",
    "Biases: Biases allow the network to shift the activation function, providing additional flexibility to fit the data. They help the network to better fit the data by adjusting the output of the neuron independent of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab809cf-e481-4f5f-b22e-6696d3d8369d",
   "metadata": {},
   "source": [
    "# Q.5\n",
    "\n",
    "The softmax function is applied in the output layer during forward propagation in multi-class classification problems. Its purpose is to convert the raw output logits (which can be any real numbers) into probabilities that sum to 1. This allows the network to make probabilistic predictions, with each class being assigned a probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93bb25-f7b5-4ea0-b3d0-c9089bfeba0d",
   "metadata": {},
   "source": [
    "# Q.6\n",
    "\n",
    "Backward propagation (or backpropagation) is the process of updating the network's weights and biases based on the error between the predicted output and the actual target. The purpose of backpropagation is to minimize the loss function, which quantifies the difference between the network's predictions and the true labels, by iteratively adjusting the parameters (weights and biases) of the network.\n",
    "\n",
    "During backpropagation, the network calculates the gradient of the loss function with respect to each weight and bias using the chain rule. These gradients are then used to update the weights and biases in a direction that reduces the loss, typically through an optimization algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfec18f-ce3f-43ad-a9da-ba95d52ce732",
   "metadata": {},
   "source": [
    "# Q.7\n",
    "\n",
    "1. Compute the Loss:\n",
    "2. Calculate the Gradient of the Loss with Respect to the Output:\n",
    "3. Apply the Chain Rule\n",
    "4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
